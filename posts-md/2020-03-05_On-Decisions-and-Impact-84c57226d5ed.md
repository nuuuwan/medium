#### Article 221 · March 5, 2020

# On Decisions and Impact

### Reflections on Performance Reviews

January, February and March are "Performance Review Season" in many companies. For bad companies, performance reviews are a mundane formality. All employees get a raise based on some arbitrary percentage. Usually not much.

For good companies, however, it is a time of deep reflection. "Did we meet expectations? Perhaps, we exceeded expectations? Maybe we didn't meet some expectations?"

## Impact

At good companies, these questions are discussed and debated, at and for every level of the company, from the company as a whole to its teams, and every employee. What the company accomplished (or "impact") is the sum of its organizations, which are the sum of its teams. And ultimately, people.

Conversely, an employees impact must be a subset of the companies impact. If the employee did something and this something is not such a subset, then that work is useless. Sadly, many employees in many companies, do a lot of useless work.

Many companies, including many good ones, anchor performance reviews on impact. "What was the impact X was supposed to make? Did X meet that?" Where X is the company as a whole, a team or a person.

## The Problem with Impact

Employees don't spend their time making impact. They spend their time making decisions. A "good decision" is one which has the best chance of yielding most impact, given the information available to make the decision. In other words, good decision making is a form of forecasting. The best decisions produce the best forecastable future.

The problem with forecasting and probability, in general, is luck. A good decision could lead to bad impact. In other words, an employee could make all the right decisions, but still, end up with the wrong impact. Just because they got unlucky. The opposite is worse. When bad decisions lead to good outcomes.

Even if the decisions lead to impact, it won't lead to impact now. This is the other problem. Usually, there is a significant lag between when decisions sow, and impact reaps.

[My former employer Facebook had performance reviews every six months. Hence, there was an incentive to pick projects which yielded impact on six months or less. This led to a shying away from longer-term projects, despite many such having more impact potential. Our fix was to endow long-term projects with shorter-term milestones. Where the milestone was a proxy for future impact. Sadly, these milestones were never valued as much as the "final impact".]

## The Solution

The solution to this problem is to stop valuing impact. And instead, evaluating decisions. Decision-based performance measurement (DBPM) must replace impact based performance measurement (IBPM). The latter, asks "What past decisions caused this present impact?" The former asks "What present decisions will cause future impact?"

![Image](https://cdn-images-1.medium.com/max/800/1*ozyVeINZf3PwAKuyFoD1Zg.png)

IBPM relies on correlation. The employee working on a project correlates with the project's impact. There is no (statistically significant) explanation on how the employee caused the impact. DBPM, on the other hand, relies on causation. When we try to determine if a decision was good, we build a causal theory on how it might cause a good outcome.

You might argue that this process of "determining causality" is complicated. It might be almost impossible to say which decision is the best option. This is precisely my point about luck. If two employees made one "impossible to determine" decision, and the other made another, and one succeeded and the other failed, why reward one and not the other?

## On Calibrations

[Performance Review Season is a hectic time for managers. At Facebook, we spent a lot of time with other managers in "calibration meetings". In a "calibration" different managers discuss the performance of their reports. The goal is to make sure that everyone has the same "baseline" on what "meeting expectation" means. A calibration is a detailed review of the past from macro-level dashboards with top-line metrics, to micro-level lines of individual code.]

Managers build "histories" on how the employee's decisions of the past "caused" the impact of the present. This process can have significant "cherry-picking". When there is a good impact, the manager can highlight employees' good decisions and hide bad. Asymmetrically, when there is lousy impact, the manager can ascribe the present to bad luck. Not bad decisions.

To be clear, good managers seldom "cherry-pick". They are as good at drawing connections between bad decisions and bad impact, as they are with good. But good managers are rare. There is also a breed of bad manager that "throws in a few bad apples". Such a manager will, from time to time, give a few examples of bad decisions, to avoid "Tommy only sings praise"-type accusations.

## How not to "Predict the Past"

But my point is not about good or bad managers. It's about the process. Even for good managers, this process of "predicting the past" is arduous. When you predict the future, you might be wrong. When you predict the past, you might certainly be wrong. But with IBPM, this is unavoidable.

DBPM solves "predicting the past" by avoiding it. There could still be calibrations. But instead of discussing the correlation between good things done and good impact, managers would purely discuss the goodness of decisions. Focus would shift from a few points of time where impact reaped, too many points of time where decisions sowed.

A DBPM calibration would consist of presenting an employee's decisions, and how good they were, without the benefit of hindsight. Such a presentation would need a lot of "real-time" present management from the manager. They would need to know what decisions the employee made. Daily.

## Contradictions

Despite what I've said, IBPM is more popular than DBPM. Why? There have been many studies on the subject. The most common reason goes something like this:

"With IBPM, it was possible to differentiate "good" employees and "bad". There was quite a lot of "signal" to do this. With DBPM, unfortunately, everyone looked much the same. How their decisions would cause future impact was difficult to predict, so everyone got, roughly, the same evaluation. Hence, employees didn't have incentives to work harder than others. And the company stagnated."

Reflecting on these studies let me to an interesting thought. What the following were true?

The "differentiation" IBPM causes leads to more activity. Not good activity, but activity none-the-less. The company, one way or the other, does not stagnate. The ones that are lucky enough to have "good activity" become "good companies". We don't hear of the ones that don't.

So what if IBPM works because we can only see the companies for which IBPM has succeeded? What if "An ecosystem of IBPM companies" worked better than "An ecosystem of DBPM companies"? Because the latter has become stagnant?

Then, we could conclude the following:

* DBPM companies have smarter employees. IBPM companies have "lucky" employees.

* DBPM companies are more likely to succeed than IBPM companies. DBPM companies owe their success to skill. IBPMs to luck.

* But IBPM company ecosystems are more likely to succeed than DBPM company ecosystems!!!

## Breaking the Contradiction

Can a company have both the cake and eat it? Can we have the benefits of both DBPM and IBPM?

It should be possible, if you accept that the benefit of IBPM is purely motivational. Being better than our peers is a fundamental social need. Can something else replace this motivation?

There are many right answers to this question. But the main barrier to DBPM is not the lack of one but the popularity of IBPM. So many people and companies believe that this system brings them success. And exclude the role luck might have played in it. Sooner or later, luck will run out. But then, they will the next IBPM company will replace then.

Hence, the bigger question is, "How do you convince people that DBPM is better? And IBPM will eventually lead to doom?"

But I've already tried to do that in this article. So if you are not convinced, maybe you could reread the article?